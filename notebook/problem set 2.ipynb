{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acceptable-enterprise",
   "metadata": {},
   "source": [
    "# 1. the sigmoid proof\n",
    "\n",
    "Show that the derivative of the sigmoid function given above is given by:\n",
    "\n",
    "    ðœ•ðœŽ(ð‘¥)\n",
    "ðœ•ð‘¥ = ðœŽ(ð‘¥)(1 âˆ’ ðœŽ(ð‘¥))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-thing",
   "metadata": {},
   "source": [
    "tests\n",
    "## remebering the sigmoid function:\n",
    "$$ \n",
    "\\partial\\sigma(x)= \\partial(\\frac{1}{1+e^{-x}})\n",
    "$$\n",
    "\n",
    "## remembering the formula for quotients\n",
    "$$\n",
    "\\frac{dy}{dx}\\left(\\frac{f(x)}{f(g)}\\right) \n",
    "=\n",
    "\\frac{\\frac{d}{dx}\\left(f(x)\\right)  g(x) -  f(X)\\frac{d}{dx}\\left(g(x)\\right) }  {[g(x)]^2}\n",
    "$$\n",
    "\n",
    "## breaking it into peices for sigmoid\n",
    "$$\n",
    "f'(x) = 0 \\newline\n",
    "g'(x) = -e^{-x}\n",
    "$$\n",
    "## spelling it out \n",
    "$$\n",
    "\\frac{0(1+e^{-x})-1(-e^{-x})            }\n",
    "     {\\left(1+e^{-x}\\right)^2}\n",
    "$$\n",
    "## result\n",
    "\n",
    "$$\n",
    "\\frac{e^{-x}}\n",
    "     {\\left(1+e^{-x}\\right)^2} \n",
    "$$\n",
    "\n",
    "## below we will verify the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "worldwide-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + e^{- x}}$"
      ],
      "text/plain": [
       "1/(1 + exp(-x))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import E, diff \n",
    "from sympy.abc import x\n",
    "expr = 1/(1+E**-x)\n",
    "#diff(a*log(a) - a + 1, x)\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "comparable-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}$"
      ],
      "text/plain": [
       "exp(-x)/(1 + exp(-x))**2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-thong",
   "metadata": {},
   "source": [
    "We have verified our differentiation but I am still unsure how to get it into the requested form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-memphis",
   "metadata": {},
   "source": [
    "# 2. testing forward propegartion to predicts\n",
    "Here are the initial weights:\n",
    "WEIGHTS  \n",
    "\n",
    "|Link from node| Link to node| Weight|\n",
    "|---|---|---|\n",
    "Input 1 |Hidden 1| 0.1\n",
    "Input 1 |Hidden 2| 0.3\n",
    "Input 1 |Hidden 3| âˆ’0.2\n",
    "Input 2 |Hidden 1| âˆ’0.4\n",
    "Input 2 |Hidden 2| 0.1\n",
    "Input 2 |Hidden 3| 0.2\n",
    "Input 3 |Hidden 1| âˆ’0.1\n",
    "Input 3 |Hidden 2| âˆ’0.2\n",
    "Input 3 |Hidden 3| 0.4\n",
    "Hidden 1 |Out 1 |0.5\n",
    "Hidden 1 |Out 2 |0.2\n",
    "Hidden 2 |Out 1 |âˆ’0.3\n",
    "Hidden 2 |Out 2 |âˆ’0.3\n",
    "Hidden 3 |Out 1 |0.2\n",
    "Hidden 3 |Out 2 |0.1\n",
    "BIAS WEIGHTS\n",
    "Layer| Weight \n",
    "Hidden| 0.2\n",
    "Output |0.3\n",
    "\n",
    "\n",
    "**steps** \n",
    "using these weights we will calculate a prediction and error for the following input  \n",
    "\n",
    "\n",
    "\n",
    "|input 1| input 2| input 3| Output 1| Output 2|\n",
    "| -     | -      | -      |    -    |   -     |\n",
    "|4|8|6|.65|.4|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-longitude",
   "metadata": {},
   "source": [
    "## populating our first weight matrix and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cardiovascular-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[[ 0.1  0.3 -0.2]\n",
      " [-0.4  0.1  0.2]\n",
      " [-0.1 -0.2  0.4]\n",
      " [ 0.2  0.2  0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 8, 6, 1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#to input -> hiden knode weights \n",
    "w =np.array([[.1,.3,-.2],[-.4,.1,.2],[-.1,-.2,.4],[.2,.2,.2]])\n",
    "print(w.shape)\n",
    "print(w)\n",
    "inp = np.array([4,8,6])\n",
    "inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-official",
   "metadata": {},
   "source": [
    "## calculating our weighted sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "limited-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2,  1. ,  3.4])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whid=inp.dot(w)\n",
    "whid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "quarterly-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity checking\n",
    "4*.1 + 8*-.4 + 6*-.1 + .2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-blake",
   "metadata": {},
   "source": [
    "## transformating hidden node with sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "indoor-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sigmoid function sigmoid\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "indirect-shanghai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying the sigmoid \n",
    "Thid = np.append(sigmoid(whid),1)\n",
    "Thid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-edwards",
   "metadata": {},
   "source": [
    "## populating our second weight table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "prime-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.2],\n",
       "       [-0.3, -0.3],\n",
       "       [ 0.2,  0.1],\n",
       "       [ 0.1,  0.1]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to out from hidden node weights\n",
    "W =np.array([[.5,.2],[-.3,-.3],[.2,.1],[.1,.1]])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-annex",
   "metadata": {},
   "source": [
    "## finding the weighted sum of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "structured-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09380619, -0.01471398])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating Weighted output\n",
    "Wout=Thid.dot(W)\n",
    "Wout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "increasing-scientist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52343437, 0.49632157])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## applying the sigmoid and getting our outputs\n",
    "Tout = sigmoid(Wout)\n",
    "Tout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-facing",
   "metadata": {},
   "source": [
    "## now putting it all together into one function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "happy-tyler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52343437, 0.49632157])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.append(sigmoid(inp.dot(w)),1).dot(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "defined-wrestling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52343437, 0.49632157])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a function for later:\n",
    "def predict(inp,w,W,func=sigmoid):\n",
    "    return func(np.append(func(inp.dot(w)),1).dot(W))\n",
    "predict(inp,w,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "happy-street",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65, 0.4 ])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = np.array([.65,.4])\n",
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "expected-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000457351603756041"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = (sum(targ - Tout)**2)*.5\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-bottom",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "controversial-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"our sigmoid activation funciton \"\"\"\n",
    "    import numpy as np #importing dependencies as is best practice \n",
    "    return 1/(1+np.e**(-x))\n",
    "\n",
    "\n",
    "def error(targ, pred):\n",
    "    \"\"\" error function\"\"\"\n",
    "   return  (sum(targ - pred)**2)*.5\n",
    "\n",
    "def predict(inp,w,W,func=sigmoid, y=None):\n",
    "    \"\"\"this function takes in inputs, weight matrix, and an activation function and makes a predication \"\"\"\n",
    "    pred = func(np.append(func(inp.dot(w)),1).dot(W))\n",
    "    if E is None: \n",
    "        return pred\n",
    "    else: \n",
    "        \n",
    "        return error(y, pred)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "light-concord",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000457351603756041"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(inp,w,W,y=targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-alexandria",
   "metadata": {},
   "source": [
    "# 3.working on backware propegaation:\n",
    "\n",
    "**our back propegation several steps:**\n",
    "1. find and store our gradients:  \n",
    "   As we are trying to minimize our errors the gradients we find are for the funciton for are from each weight and bias to the Error term.  \n",
    "    1. first step find gradients for output to hidden\n",
    "        1. for all hidden nodes and output nodes Gradiants for\n",
    "        2. for BiasO for all output nodes\n",
    "    3. second step find gradients for hidden to input\n",
    "        1. for all hidden nodes and inputs \n",
    "        2. for BiasH for all hidden nodes \n",
    "    4.  use calcualted gradients to apply update to weights\n",
    "        \n",
    "\n",
    "**functions and informaiton we need to power our gradients:**\n",
    "\n",
    "* input information: inputs, target outputs, weight matrix's \n",
    "* cacluated iformaiton: \n",
    "    * Thid - our activated hiden nodes values  with respect to our input \n",
    "    * Tout - our predictations with respect to our input \n",
    "* funcs\"\n",
    "    * derivit funciton of our error function with respect to E \n",
    "    * derivitive funciton of  our activatiion function  \n",
    "\n",
    "\n",
    "**Detailed steps:**\n",
    "1. initalize matrix's g and G of shape IxH and HxO where I is Num_inputs, H is Num_hideen nodes, and O is num_ outputs \n",
    "2. initializ 2 bias arrays if len H and O \n",
    "3. use a nested for loop to cacluate for all weights between all hidden and output noodes the gradient with resepect to each weight:\n",
    "    3. first caculate the Bias term for kth output node\n",
    "    4. use the bias term for k and multipy by the jth hidden node to  to find the gradeints each hidden node j output node k \n",
    "    5. store the biask for use in later caculatations\n",
    "        * special care has to be taken here biases must be summed before being utlized in gradiant updates\n",
    "4. use a nested for loop to calculate for all weights between  all input and hidden noedes the gradient with repect to each weight:\n",
    "    1. for each hidden node j and bias k from the previous step find the dot product between weight vector and biases\n",
    "    2. find the bias for each hidden node j\n",
    "    3. use the bias for hidden node j and multiply for each input i to find the gradient for each weight i,j \n",
    "    4. update Biash with bias term\n",
    "5. update gradient matrixs\n",
    "    1. for hidden to output gradient matrix\n",
    "        1. sum bias vector and then appened to matrix\n",
    "    2. for input to hiddne gradient matrix\n",
    "        1. append bias vector \n",
    "5. apply gradients to weights using formula $weight-\\alpha*gradient$ we will use alpha .2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-profit",
   "metadata": {},
   "source": [
    "## initalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "guided-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 3\n",
    "num_hidden = 3\n",
    "num_outputs = 2\n",
    "\n",
    "# intialize matrix and vectors\n",
    "G = np.zeros((num_hidden,num_outputs))\n",
    "g = np.zeros((num_inputs,num_hidden))\n",
    "Bo = np.zeros((1,num_outputs))\n",
    "Bh = np.zeros((1,num_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "experienced-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our derivitive error function\n",
    "def Der_error(pred,target):\n",
    "    return pred-target\n",
    "# set up our derivitive activation function\n",
    "def Der_sigmoid(val):\n",
    "    return val *(1-val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-lawsuit",
   "metadata": {},
   "source": [
    "## first step back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "verbal-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.loop and cacluate gradiants and  bias for first step back \n",
    "\n",
    "\n",
    "for k in range(num_outputs):\n",
    "    #caculate for an output node \n",
    "    Bias = Der_error(Tout[k],targ[k])*Der_sigmoid(Tout[k])\n",
    "    for j in range(num_hidden):\n",
    "        #add for hidden node \n",
    "        G[j,k]=Bias*Thid[j]\n",
    "    \n",
    "    #upate bias vectore\n",
    "    Bo[0,k] = Bias #note this will be summed before utlized in updates but information stored here can be utlized in next step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-sullivan",
   "metadata": {},
   "source": [
    "## second step back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "further-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop and caculate gradients and  bias for second step back\n",
    "for k in range(num_hidden):\n",
    "    bias = Bo.dot(W[k,:])*Der_sigmoid(Thid[k])\n",
    "    for j in range(num_inputs):\n",
    "        g[j,k] = bias*inp[j]\n",
    "    Bh += bias #updating all biaas because we aren't using these values again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-nutrition",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "pending-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up gradient arrays \n",
    "\n",
    "#sum Bo\n",
    "Bo[:] = np.sum(Bo)\n",
    "G = np.append(G,Bo,axis=0)\n",
    "g = np.append(g,Bh,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "annoying-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update weights \n",
    "\n",
    "W_new = W-.2*G\n",
    "w_new = w-.2*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "conventional-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10033026,  0.29964644, -0.19990233],\n",
       "       [-0.39933948,  0.09929288,  0.20019534],\n",
       "       [-0.09950461, -0.20053034,  0.4001465 ],\n",
       "       [ 0.20001859,  0.20001859,  0.20001859]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-planning",
   "metadata": {},
   "source": [
    "# 4. bulding everything together\n",
    "\n",
    "we will now take all the steps above and encode them into a CLass object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class neural_network:\n",
    "    __init__(self,num_Inputs,num_Outputs,num_HiddenNodes,errorfunc=Square_error,activation=sigmoid):\n",
    "        \n",
    "        \n",
    "        self._num_Inputs =num_Inputs\n",
    "        self._num_Outputs = num_Outputs\n",
    "        self._num_HiddenNodes = num_HiddenNodes\n",
    "        \n",
    "        if errorfunc ==Square_error:\n",
    "            self.errorfunc = Square_error\n",
    "            self.Der_errorfunc = Der_Square_error\n",
    "        \n",
    "        if activation == sigmoid:\n",
    "            self.actfunc = sigmoid\n",
    "            self.Der_evtfunc = Der_sigmoid\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "        self.w = rng.(-5,5,size=num_HiddenNodes*num_Inputs)\n",
    "        self.w.reshape((num_inputs,num_HiddenNodes))\n",
    "        \n",
    "        \n",
    "    def matrix_initalization(self):\n",
    "        rng = np.random.default_rng()\n",
    "        w = rng.integers(-5,5,size=self._num_HiddenNodes*self._num_Inputs)\n",
    "        self.w = w.reshape((num_inputs,num_HiddenNodes))\n",
    "        \n",
    "        W = rng.integers(-5,5,size=self._num_HiddenNodes*self._num_Outputs)\n",
    "        self.W = W.reshape(self._)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
