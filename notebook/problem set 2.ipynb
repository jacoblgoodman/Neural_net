{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geological-bennett",
   "metadata": {},
   "source": [
    "# 1. the sigmoid proof\n",
    "\n",
    "Show that the derivative of the sigmoid function given above is given by:\n",
    "\n",
    "    𝜕𝜎(𝑥)\n",
    "𝜕𝑥 = 𝜎(𝑥)(1 − 𝜎(𝑥))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-eugene",
   "metadata": {},
   "source": [
    "tests\n",
    "## remebering the sigmoid function:\n",
    "$$ \n",
    "\\partial\\sigma(x)= \\partial(\\frac{1}{1+e^{-x}})\n",
    "$$\n",
    "\n",
    "## remembering the formula for quotients\n",
    "$$\n",
    "\\frac{dy}{dx}\\left(\\frac{f(x)}{f(g)}\\right) \n",
    "=\n",
    "\\frac{\\frac{d}{dx}\\left(f(x)\\right)  g(x) -  f(X)\\frac{d}{dx}\\left(g(x)\\right) }  {[g(x)]^2}\n",
    "$$\n",
    "\n",
    "## breaking it into peices for sigmoid\n",
    "$$\n",
    "f'(x) = 0 \\newline\n",
    "g'(x) = -e^{-x}\n",
    "$$\n",
    "## spelling it out \n",
    "$$\n",
    "\\frac{0(1+e^{-x})-1(-e^{-x})            }\n",
    "     {\\left(1+e^{-x}\\right)^2}\n",
    "$$\n",
    "## result\n",
    "\n",
    "$$\n",
    "\\frac{e^{-x}}\n",
    "     {\\left(1+e^{-x}\\right)^2} \n",
    "$$\n",
    "\n",
    "## below we will verify the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1/(1 + exp(-x))",
      "text/latex": "$\\displaystyle \\frac{1}{1 + e^{- x}}$"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import E, diff \n",
    "from sympy.abc import x\n",
    "import numpy as np\n",
    "expr = 1/(1+E**-x)\n",
    "#diff(a*log(a) - a + 1, x)\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cutting-aviation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "exp(-x)/(1 + exp(-x))**2",
      "text/latex": "$\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}$"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-submission",
   "metadata": {},
   "source": [
    "We have verified our differentiation but I am still unsure how to get it into the requested form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-conspiracy",
   "metadata": {},
   "source": [
    "# 2. testing forward propegartion to predicts\n",
    "Here are the initial weights:\n",
    "WEIGHTS  \n",
    "\n",
    "| Link from node | Link to node | Weight |\n",
    "|----------------|--------------|--------|\n",
    "| Input 1        | Hidden 1     | 0.1    |\n",
    "| Input 1        | Hidden 2     | 0.3    |\n",
    "| Input 1        | Hidden 3     | −0.2   |\n",
    "| Input 2        | Hidden 1     | −0.4   |\n",
    "| Input 2        | Hidden 2     | 0.1    |\n",
    "| Input 2        | Hidden 3     | 0.2    |\n",
    "| Input 3        | Hidden 1     | −0.1   |\n",
    "| Input 3        | Hidden 2     | −0.2   |\n",
    "| Input 3        | Hidden 3     | 0.4    |\n",
    "| Hidden 1       | Out 1        | 0.5    |\n",
    "| Hidden 1       | Out 2        | 0.2    |\n",
    "| Hidden 2       | Out 1        | −0.3   |\n",
    "| Hidden 2       | Out 2        | −0.3   |\n",
    "| Hidden 3       | Out 1        | 0.2    |\n",
    "| Hidden 3       | Out 2        | 0.1    |\n",
    "|                | BIAS WEIGHTS |        |\n",
    "|                | Layer        | Weight |\n",
    "|                | Hidden       | 0.2    |\n",
    "|                | Output       | 0.3    |\n",
    "\n",
    "\n",
    "**steps** \n",
    "using these weights we will calculate a prediction and error for the following input  \n",
    "\n",
    "\n",
    "\n",
    "| input 1 | input 2 | input 3 | Output 1 | Output 2 |\n",
    "|---------|---------|---------|----------|----------|\n",
    "| 4       | 8       | 6       | .65      | .4       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-absolute",
   "metadata": {},
   "source": [
    "## populating our first weight matrix and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[[ 0.1  0.3 -0.2]\n",
      " [-0.4  0.1  0.2]\n",
      " [-0.1 -0.2  0.4]\n",
      " [ 0.2  0.2  0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([4, 8, 6, 1])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#to input -> hiden knode weights \n",
    "w =np.array([[.1,.3,-.2],[-.4,.1,.2],[-.1,-.2,.4],[.2,.2,.2]])\n",
    "print(w.shape)\n",
    "print(w)\n",
    "inp = np.array([4,8,6,1])\n",
    "inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-stereo",
   "metadata": {},
   "source": [
    "## calculating our weighted sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-france",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-3.2,  1. ,  3.4])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whid=inp.dot(w)\n",
    "whid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "understanding-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-3.2"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity checking\n",
    "4*.1 + 8*-.4 + 6*-.1 + .2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-blogger",
   "metadata": {},
   "source": [
    "## transformating hidden node with sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sigmoid function sigmoid\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quiet-appearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4,)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying the sigmoid \n",
    "Thid = np.append(sigmoid(whid),1)\n",
    "Thid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-accounting",
   "metadata": {},
   "source": [
    "## populating our second weight table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continent-wholesale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.5,  0.2],\n       [-0.3, -0.3],\n       [ 0.2,  0.1],\n       [ 0.3,  0.3]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to out from hidden node weights\n",
    "W =np.array([[.5,.2],[-.3,-.3],[.2,.1],[.3,.3]])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-contrast",
   "metadata": {},
   "source": [
    "## finding the weighted sum of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "concerned-branch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.29380619, 0.18528602])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating Weighted output\n",
    "Wout=Thid.dot(W)\n",
    "Wout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "significant-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5729277 , 0.54618944])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## applying the sigmoid and getting our outputs\n",
    "Tout = sigmoid(Wout)\n",
    "Tout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-china",
   "metadata": {},
   "source": [
    "## now putting it all together into one function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numeric-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5729277 , 0.54618944])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.append(sigmoid(inp.dot(w)),1).dot(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "palestinian-window",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5729277 , 0.54618944])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a function for later:\n",
    "def predict(inp,w,W,func=sigmoid):\n",
    "    return func(np.append(func(inp.dot(w)),1).dot(W))\n",
    "predict(inp,w,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intellectual-pendant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.65, 0.4 ])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = np.array([.65,.4])\n",
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rural-glory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.0023885891017721714"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = (sum(targ - Tout)**2)*.5\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-basketball",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "extended-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"our sigmoid activation funciton \"\"\"\n",
    "    import numpy as np #importing dependencies as is best practice \n",
    "    return 1/(1+np.e**(-x))\n",
    "\n",
    "\n",
    "def error(targ, pred):\n",
    "    \"\"\" error function\"\"\"\n",
    "    return  (sum(targ - pred)**2)*.5\n",
    "\n",
    "def predict(inp,w,W,func=sigmoid, y=None):\n",
    "    \"\"\"this function takes in inputs, weight matrix, and an activation function and makes a predication \"\"\"\n",
    "    pred = func(np.append(func(inp.dot(w)),1).dot(W))\n",
    "    if E is None: \n",
    "        return pred\n",
    "    else: \n",
    "        \n",
    "        return error(y, pred)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minimal-intro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.0023885891017721714"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(inp,w,W,y=targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-dimension",
   "metadata": {},
   "source": [
    "# 3.working on backware propegaation:\n",
    "\n",
    "**our back propegation several steps:**\n",
    "1. find and store our gradients:  \n",
    "   As we are trying to minimize our errors the gradients we find are for the funciton for are from each weight and bias to the Error term.  \n",
    "    1. first step find gradients for output to hidden\n",
    "        1. for all hidden nodes and output nodes Gradiants for\n",
    "        2. for BiasO for all output nodes\n",
    "    3. second step find gradients for hidden to input\n",
    "        1. for all hidden nodes and inputs \n",
    "        2. for BiasH for all hidden nodes \n",
    "    4.  use calcualted gradients to apply update to weights\n",
    "        \n",
    "\n",
    "**functions and informaiton we need to power our gradients:**\n",
    "\n",
    "* input information: inputs, target outputs, weight matrix's \n",
    "* cacluated iformaiton: \n",
    "    * Thid - our activated hiden nodes values  with respect to our input \n",
    "    * Tout - our predictations with respect to our input \n",
    "* funcs\"\n",
    "    * derivit funciton of our error function with respect to E \n",
    "    * derivitive funciton of  our activatiion function  \n",
    "\n",
    "\n",
    "**Detailed steps:**\n",
    "1. initalize matrix's g and G of shape IxH and HxO where I is Num_inputs, H is Num_hideen nodes, and O is num_ outputs \n",
    "2. initializ 2 bias arrays if len H and O \n",
    "3. use a nested for loop to cacluate for all weights between all hidden and output noodes the gradient with resepect to each weight:\n",
    "    3. first caculate the Bias term for kth output node\n",
    "    4. use the bias term for k and multipy by the jth hidden node to  to find the gradeints each hidden node j output node k \n",
    "    5. store the biask for use in later caculatations\n",
    "        * special care has to be taken here biases must be summed before being utlized in gradiant updates\n",
    "4. use a nested for loop to calculate for all weights between  all input and hidden noedes the gradient with repect to each weight:\n",
    "    1. for each hidden node j and bias k from the previous step find the dot product between weight vector and biases\n",
    "    2. find the bias for each hidden node j\n",
    "    3. use the bias for hidden node j and multiply for each input i to find the gradient for each weight i,j \n",
    "    4. update Biash with bias term\n",
    "5. update gradient matrixs\n",
    "    1. for hidden to output gradient matrix\n",
    "        1. sum bias vector and then appened to matrix\n",
    "    2. for input to hiddne gradient matrix\n",
    "        1. append bias vector \n",
    "5. apply gradients to weights using formula $weight-\\alpha*gradient$ we will use alpha .2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-seminar",
   "metadata": {},
   "source": [
    "## initalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daily-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 3\n",
    "num_hidden = 3\n",
    "num_outputs = 2\n",
    "\n",
    "# intialize matrix and vectors\n",
    "G = np.zeros((num_hidden,num_outputs))\n",
    "g = np.zeros((num_inputs,num_hidden))\n",
    "Bo = np.zeros((1,num_outputs))\n",
    "Bh = np.zeros((1,num_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coral-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our derivitive error function\n",
    "def Der_error(pred,target):\n",
    "    return pred-target\n",
    "# set up our derivitive activation function\n",
    "def Der_sigmoid(val):\n",
    "    return val *(1-val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-dynamics",
   "metadata": {},
   "source": [
    "## first step back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "injured-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.loop and cacluate gradiants and  bias for first step back \n",
    "\n",
    "\n",
    "for k in range(num_outputs):\n",
    "    #caculate for an output node \n",
    "    Bias = Der_error(Tout[k],targ[k])*Der_sigmoid(Tout[k])\n",
    "    for j in range(num_hidden):\n",
    "        #add for hidden node \n",
    "        G[j,k]=Bias*Thid[j]\n",
    "    \n",
    "    #upate bias vectore\n",
    "    Bo[0,k] = Bias #note this will be summed before utlized in updates but information stored here can be utlized in next step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-simulation",
   "metadata": {},
   "source": [
    "## second step back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "considerable-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop and caculate gradients and  bias for second step back\n",
    "for k in range(num_hidden):\n",
    "    bias = Bo.dot(W[k,:])*Der_sigmoid(Thid[k])\n",
    "    for j in range(num_inputs):\n",
    "        g[j,k] = bias*inp[j]\n",
    "    Bh += bias #updating all biaas because we aren't using these values again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-lindsay",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "speaking-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up gradient arrays \n",
    "\n",
    "#sum Bo\n",
    "Bo[:] = np.sum(Bo)\n",
    "G = np.append(G,Bo,axis=0)\n",
    "g = np.append(g,Bh,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instant-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update weights \n",
    "\n",
    "W_new = W-.2*G\n",
    "w_new = w-.2*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "flush-conducting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.10006569,  0.30081998, -0.1999963 ],\n       [-0.39986862,  0.10163996,  0.2000074 ],\n       [-0.09990147, -0.19877003,  0.40000555],\n       [ 0.20022234,  0.20022234,  0.20022234]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-longitude",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. bulding everything together\n",
    "\n",
    "we will now take all the steps above a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "num_inputs = 3\n",
    "num_hidden = 3\n",
    "num_outputs = 2\n",
    "def get_gradients(x, y, alpha =.2, batch=False):\n",
    "    # initalize\n",
    "    G = np.zeros((num_hidden, num_outputs))\n",
    "    g = np.zeros((num_inputs, num_hidden))\n",
    "    Bo = np.zeros((1,num_outputs))\n",
    "    Bh = np.zeros((1,num_hidden))\n",
    "\n",
    "    #pulling weights from global scope\n",
    "    global W\n",
    "    global w\n",
    "\n",
    "    # create predictions\n",
    "    cur_pred = predict(x, w=w, W=W)\n",
    "\n",
    "    # first step loop\n",
    "    for k in range(num_outputs):\n",
    "        # calculate for an output node\n",
    "        pBias = Der_error(cur_pred[k],y[k]) *  Der_sigmoid(cur_pred[k])\n",
    "        for j in range(num_hidden):\n",
    "            # add for hidden node\n",
    "            G[j, k] = pBias*Thid[j]\n",
    "\n",
    "        # update bias vector\n",
    "        Bo[0, k] = pBias     # note this will be summed before utlized in updates but information stored\n",
    "        # here can be utlized in next step\n",
    "\n",
    "    # second step loop\n",
    "    for k in range(num_hidden):\n",
    "        pbias = Bo.dot(W[k,:]) * Der_sigmoid(Thid[k])\n",
    "        for j in range(num_inputs):\n",
    "            g[j, k] = pbias*x[j]\n",
    "        Bh += pbias  # updating all bias because we aren't using these values again\n",
    "\n",
    "    # clean up gradient arrays and append bias's\n",
    "    Bo[:] = np.sum(Bo)\n",
    "    G = np.append(G,Bo,axis=0)\n",
    "    g = np.append(g,Bh,axis=0)\n",
    "\n",
    "    if batch is True:\n",
    "        print(\"one pass\")\n",
    "        return g, G\n",
    "    elif batch is False:\n",
    "\n",
    "        W = W - alpha*G\n",
    "\n",
    "        w = w - alpha*g\n",
    "        return print('one pass weights updated')\n",
    "\n",
    "    else:\n",
    "        return ('bad options for batch should be True/False')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one pass\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[-3.28448805e-04, -4.09990114e-03, -1.85123644e-05],\n        [-6.56897609e-04, -8.19980228e-03, -3.70247289e-05],\n        [-4.92673207e-04, -6.14985171e-03, -2.77685467e-05],\n        [-1.11171558e-03, -1.11171558e-03, -1.11171558e-03]]),\n array([[-0.00073859,  0.00141919],\n        [-0.01378643,  0.02649025],\n        [-0.01824914,  0.03506523],\n        [ 0.0173773 ,  0.0173773 ]]))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradients(inp,targ, batch=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "inp_many = np.tile(inp,3).reshape((-1,4))\n",
    "\n",
    "targ_many = np.tile(targ,3).reshape((-1,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int32' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-61-4ba86e6fb1ec>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtarg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-33-59552c719d81>\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(inp, w, W, func)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# building a function for later:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.int32' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "for x, y in zip(inp,targ):\n",
    "    print(error(y,predict(x,w,W)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}